---
title: "Introduction To Data Mining"
author: "Joshua de la Bruere"
date: "3/30/2020"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
description: "Welcome to Data Mining With R!"
---

```{r setup, include=FALSE}
library(learnr)
#tutorial_options(exercise.timelimit = 60)
```


## Overview Of the Course
### Topics Covered
This course will cover the following topics:

+ Overview of Data Mining
+ Using visualization for Data Mining
+ Supervised Classification
+ Unsupervised Classification
+ Regression (linear/Logistic)
+ Neural Networks
+ Decision Trees
+ K Nearest neighbors
+ Random Forests
+ Using Tidy Models

### Is this course for you? 

Is this course a good fit for you? Answer these questions to find out. 

```{r course_fit_prereq, echo = FALSE}
question("Are you familiar with the following?", 
         answer("data manipulation in R", correct = T, message = "* Data Manipulation is a required skill"), 
         answer("the Tidyverse", correct = T, message = "* The Tidyverse will be used often in this course"), 
         answer("the concept of Tidy Data", correct = T), 
         type = "multiple", 
         incorrect = "It looks like you're missing a few prerequisits, this course may not be a good fit."
         )

```

```{r course_fit_topics, echo = FALSE}

question("Do you already know how to do the following in R?", 
         answer("creating models in R", message = "This will be covered"), 
         answer("classification, Regression, Neural Nets, and Association Rules" ), 
         answer("None of these", correct = T, message = "Have no fear, these will be covered in this course."),
         type = "multiple",
         incorrect =  "This tutorial will be covering things that you're already familiar with."
         )
```



## Core Ideas in Data Mining

Data Mining is a wide subject that contains many smaller subject areas. Often only the cutting edge machine learning methods are thought about, but data mining encompasess other areas of data science such as  data exploration and visualization, in addition to the traditional modeling aspect of data mining. 


### Data Exploration

Data rarely comes in a neatly packaged data set. It's not uncommon to have huge data sets that are derived from disparte and poorly linked data sources. These sources may be incomplete creating missing data. They may be of poor quality, with all manner of typos, and issues. **Data Exploration** enables you to detect these issues early as improving the quality of your data is one of the best ways to improve your ability to create a successful model. After all, if you feed your algorithim garbage, you're likely going to get garbage back out.

<center>
![XKCD Comic](https://imgs.xkcd.com/comics/machine_learning.png)
</center>


Data Exploration is a set of techniques to understand your data, what it contains, what it's missing, and the shape and scale of the data. These techniques can be as simple as some light visualization, to as complicated as data imputation. These Topics will be explored more in depth after the overview.

### Data Visulization

### Modeling

+ Supervised
+ Unsupervised

### Implementation

### Common Issues




## Data Exploration and Cleaning

One of the most important steps in data mining is getting to know your data. You want to have a general understanding of how the data is formed, what types of data you have, any outliers, or missing data. Knowing how dense your data is will help determine what data mining techniques you can use, as well as what sort or clean up or preprocessing you will need. 

### Types of Variables

Data types can be divided (with a very broad brush) into two categories, numeric and Categorical variables. Sometimes these are also refered to as qualititve and quantititave data respectivly. Most algorithms are designed around numeric data, though some can handel categorical data as well. Sometimes a transformation will be needed to turn categorical data into a numerical proxy. 

#### Quantitative Data 
Broadly speaking quantitative (numeric) data can be further broken down into continuous data and discrete data. An easy distinction here is that continous data is something that can be measured like cm, or volume. There's an infinite number of potentail values that a continous variable could be. 

The other side of the quantitative coin is discrete data. Discrete data has a finite number of possible values, think for example number of students in a room, you can't have half a student, so this is a discrete number. 


```{r continous vs discrete, echo=FALSE}
#data

```

#### Qualititaive Data

#### Check Your Understanding

```{r data types questions,  echo = FALSE}


```

### Detecting Outliers

#### What is an Outlier?

An **Outlier** is an "exteme observation, meaning that it is distant or seemingly an oddity compared to the rest of the data. Distant is purposuly vague, as distant can be different depending on the data set being worked with. They are not nessicarily an error or mistake. Sometimes, the data is truly exteme.

Outliers can have a disproportianant effect on models, which can be a problem if the effect it has is not a valid conclusion. 

##### Example of suprious outlier

##### Animated/interactive example of the effect of outliers

As they can have a negitive effect, detecting outliers is an important preprocessing step in data mining, by detecting the outliers, one can take steps to either correct them (if they're suprious), or omit them if they are negativly effecting the model. Before taking action on the outliers, it's best to consult a domain expert, as they will often have the knoweledge to determine if it's an error, or if the recording is correct.

In some cases, detecting the outliers can be the purpose of data mining. When this is the case, it is called **Anomaly Detection". Think of airport security screening, where you might detect if someone has been making trips to strange countries, or the same countries too often. If an outlier like that exsists, they could be a potential threat or smuggling something.  

#### Methods of detecting outliers

### Dealing With Missing Data

Sometimes, you just don't have all the answers. Missing values are common and can cause issues with the data mining techniques you're using. 

#### Omission

Dealing with missing data sometimes is a strange game, the only winning move is not to play. If the number of entries in your data set missing data are few, it can be acceptable to drop those missing data. If it appears to be many entries on a few variables you can drop those variables as well. 

However, if there are many observations missing many different values, omission could decimate your data set and cause you to have non useful results. 

#### Imputation

In the cases where omission isn't practical or would be detimental to data mining, you can always try imputation; the process of replacing missing values with reasonable subsitutes. Sometimes this can be as simple as replacing the missing values with a value like the median. Domain knowledge is key here, for determining what a reasonable replacement can be. 


##### Example of Imputation 

### Normalization of Data

Normalizing (also known as standardizing) data is a common technique that is used when varialbes with larger scales would have undue influence over the resutls of your modeling. This is a process that will put all the varibles on the same scale. 


This can be achived by subtracting the mean from each value and dividing by the standard deviation.

$\frac{(x - \overline{x})}{s}$


We can write a simple function that does this, just so we can see how this can be solved. 
```{r normalizing1}

norm1 <- function(variable){
  var_mean = mean(variable)
  var_std = sd(variable)


  normalized_values = (variable-var_mean)/var_std
  return(normalized_values)
}
```


Alternativly you can scale the variables from 0 to 1 by subtracting the minimum and dividing by the range.
standardized value $ = \frac{(x - min(X))}{s}$

Again, we can right a function that does this for use for the whole column. 

```{r normalizing2}
norm2 <- function(variable){
  var_min = min(variable)
  var_max = max(variable)
  var_range = var_max-var_min


  normalized_values = (variable-var_min)/var_range
  return(normalized_values)
}
```


Go ahead and try out these functions here. 

```{r norm_example, exercise=TRUE}

# Normalize me
me = c(1,54,8,3,6,32,7,3,12,8,54,2,6)
```




